# -*- coding: utf-8 -*-
"""lstm+word2vec.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yGCKuzQtOyHUQfMvRdG1rkT5QQvhXPiB
"""

import pandas as pd
import nltk
import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import matplotlib.pyplot as plt
from keras.preprocessing.text import Tokenizer
from keras.models import Sequential
from keras.layers import Dense, LSTM, Embedding
from gensim.models import Word2Vec
from keras.utils import np_utils
from keras.preprocessing import sequence
import numpy as np
from gensim.models import FastText
import tensorflow as tf
from gensim.models import Word2Vec


nltk.download('popular')


data = pd.read_csv(r'/tweet_data.csv',encoding='latin1')



#data['class'].value_counts().plot(kind='bar')

X_data = data['tweet']
Y_data = data['class']

# 불용어 처리
result = []
stop_words = set(stopwords.words('english'))
stop_words.add('rt')
stop_words.add('u')
stop_words.add('r')
stop_words.add('n')


for tweet in X_data.values:
  tweet = tweet.lower()
  only_en = re.sub('[^a-zA-Z]', ' ', tweet)
  only_en = word_tokenize(only_en)
 
  no_stops = [word for word in only_en if not word in stop_words]
  result.append(no_stops)

token = Tokenizer()
token.fit_on_texts(result)
sequences = token.texts_to_sequences(result)

word_index = token.word_index

index_to_word={}
for key, value in word_index.items():
  index_to_word[value] = key

# 훈련용, 테스트용 분류
n_of_train = int(len(data) * 0.8)
n_of_test = int(len(data) - n_of_train)

X_data = sequences
#max_len = max(len(l) for l in X_data)
max_len = 29

for i in range(len(X_data)):
  for j in range(len(X_data[i])):
    X_data[i][j] -= 1

# 데이터 패딩
x_train = sequence.pad_sequences(X_data[:n_of_train], maxlen=max_len)
x_test = sequence.pad_sequences(X_data[n_of_train:], maxlen=max_len)

y_train = np_utils.to_categorical(Y_data[:n_of_train])
y_test = np_utils.to_categorical(Y_data[n_of_train:])

# Word2Vec 임베딩
ft_model = FastText(result, size=120, window=5, min_count=0, workers=4, sg=1)

ft_weights = ft_model.wv.vectors
voca_size = ft_weights.shape[0]
embedding_size = ft_weights.shape[1]

embedding_matrix = np.zeros((voca_size, embedding_size))
for word, i in token.word_index.items():
    if word in ft_model.wv:
        embedding_vector = ft_model.wv[word]
        
        if embedding_vector is not None:
            embedding_matrix[i-1] = embedding_vector


# lstm 모델
model = Sequential()
model.add(Embedding(input_dim=voca_size,
                    output_dim=embedding_size,
                    weights=[embedding_matrix],
                    input_length=max_len,
                    mask_zero=True,
                    trainable=False))

model.add(LSTM(embedding_size, input_shape=(5, 2)))
model.add(Dense(3, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

history = model.fit(x_train, y_train, batch_size=2000, epochs=20, validation_data=(x_train[:10000], y_train[:10000]))

epochs = range(1, len(history.history['acc']) + 1)
model.summary()
print(history.history['val_loss'])
plt.plot(epochs, history.history['loss'])
plt.plot(epochs, history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

print("\n 정확도 : %.4f" % (model.evaluate(x_test, y_test)[1]))