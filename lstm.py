# -*- coding: utf-8 -*-
"""lstm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lTk7mS1ny1YX1sdUjFCWf64oqyVPk9F2
"""

import pandas as pd
import nltk
import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import matplotlib.pyplot as plt
from keras.preprocessing.text import Tokenizer
from keras.models import Sequential
from keras.layers import Dense, LSTM, Embedding
from gensim.models import Word2Vec
from keras.utils import np_utils
from keras.preprocessing import sequence
import numpy
import tensorflow as tf



nltk.download('popular')


data = pd.read_csv(r'/tweet_data.csv',encoding='latin1')



#data['class'].value_counts().plot(kind='bar')

X_data = data['tweet']
Y_data = data['class']

# 불용어 처리
result = []
stop_words = set(stopwords.words('english'))
stop_words.add('rt')
stop_words.add('u')
stop_words.add('r')
stop_words.add('n')


for tweet in X_data.values:
  tweet = tweet.lower()
  only_en = re.sub('[^a-zA-Z]', ' ', tweet)
  only_en = word_tokenize(only_en)
 
  no_stops = [word for word in only_en if not word in stop_words]
  result.append(no_stops)

# 단어 인덱싱
token = Tokenizer()
token.fit_on_texts(result)
sequences = token.texts_to_sequences(result)

word_index = token.word_index

index_to_word={}
for key, value in word_index.items():
  index_to_word[value] = key

# 훈련용, 테스트용 분류
n_of_train = int(len(data) * 0.8)
n_of_test = int(len(data) - n_of_train)

X_data = sequences
max_len = max(len(l) for l in X_data)
for i in range(len(X_data)):
  for j in range(len(X_data[i])):
    X_data[i][j] -= 1

# 데이터 패딩
x_train = sequence.pad_sequences(X_data[:n_of_train], maxlen=max_len)
x_test = sequence.pad_sequences(X_data[n_of_train:], maxlen=max_len)

# 원-핫 인코딩
y_train = np_utils.to_categorical(Y_data[:n_of_train])
y_test = np_utils.to_categorical(Y_data[n_of_train:])

# lstm 모델 생성
model = Sequential()
model.add(Embedding(len(index_to_word),120))
model.add(LSTM(120, input_shape=(5, 2)))
model.add(Dense(3, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

model.summary()
history = model.fit(x_train, y_train, batch_size=2000, epochs=20, validation_data=(x_train[:10000], y_train[:10000]))

epochs = range(1, len(history.history['acc']) + 1)
print(history.history['val_loss'])

plt.plot(epochs, history.history['loss'])
plt.plot(epochs, history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

print("\n 정확도 : %.4f" % (model.evaluate(x_test, y_test)))