# -*- coding: utf-8 -*-
"""word2vec+lstm+cnn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MgfomCu1YSxdFngHiUovobR436WH-sTJ
"""

import pandas as pd
import nltk
import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import matplotlib.pyplot as plt
from keras.preprocessing.text import Tokenizer
from keras.models import Sequential
from keras.layers import Dense, LSTM, Embedding, Flatten, Dropout, Conv1D, MaxPooling1D, Input
from gensim.models import Word2Vec
from keras.utils import np_utils
from keras.preprocessing import sequence
import numpy as np
import tensorflow as tf



nltk.download('popular')


data = pd.read_csv(r'/tweet_data.csv',encoding='latin1')



#data['class'].value_counts().plot(kind='bar')

X_data = data['tweet']
print(len(X_data))
Y_data = data['class']

# 불용어 처리
result = []
stop_words = set(stopwords.words('english'))
stop_words.add('rt')
stop_words.add('u')
stop_words.add('r')
stop_words.add('n')


for tweet in X_data.values:
  tweet = tweet.lower()
  only_en = re.sub('[^a-zA-Z]', ' ', tweet)
  only_en = word_tokenize(only_en)
 
  no_stops = [word for word in only_en if not word in stop_words]
  result.append(no_stops)

# 단어 인덱싱
token = Tokenizer()
token.fit_on_texts(result)
sequences = token.texts_to_sequences(result)

word_index = token.word_index

index_to_word={}
for key, value in word_index.items():
  index_to_word[value] = key

# 훈련용, 테스트용 분류
n_of_train = int(len(data) * 0.8)
n_of_test = int(len(data) - n_of_train)

X_data = sequences
max_len = max(len(l) for l in X_data)
for i in range(len(X_data)):
  for j in range(len(X_data[i])):
    X_data[i][j] -= 1
    
# 데이터 패딩
x_train = sequence.pad_sequences(X_data[n_of_train:], maxlen=max_len)
x_test = sequence.pad_sequences(X_data[:n_of_train], maxlen=max_len)

# 원-핫 인코딩
y_train = np_utils.to_categorical(Y_data[n_of_train:])
y_test = np_utils.to_categorical(Y_data[:n_of_train])

# word2vec 
w2v_model = Word2Vec(sentences=result, size=120, window=5, min_count=0, workers=4, sg=1)

w2v_weights = w2v_model.wv.vectors
voca_size = w2v_weights.shape[0]
embedding_size = w2v_weights.shape[1]

embedding_matrix = np.zeros((voca_size, embedding_size))

for word, i in token.word_index.items():
    if word in w2v_model.wv:
        embedding_vector = w2v_model.wv[word]
        
        if embedding_vector is not None:
            embedding_matrix[i-1] = embedding_vector

# cnn 모델 생성
model = Sequential()
model.add(Embedding(input_dim=voca_size,
                    output_dim=embedding_size,
                    weights=[embedding_matrix],
                    input_length=max_len,
                    trainable=False))

model.add(Conv1D(128, 2, activation='relu'))
model.add(MaxPooling1D(pool_size=4, strides=3))
model.add(Dropout(0.25))
model.add(Conv1D(64, 3, activation='relu'))
model.add(MaxPooling1D(pool_size=3, strides=2))
model.add(Dropout(0.25))
model.add(Conv1D(32, 2, activation='relu'))
model.add(MaxPooling1D(pool_size=2, strides=1))
model.add(Dropout(0.25))
model.add(LSTM(128,input_shape=(5, 2)))
model.add(Dropout(0.25))
model.add(Dense(3, activation='softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()
history = model.fit(x_train, y_train, batch_size=500, epochs=50, validation_data=(x_test, y_test))
print(history.history['val_loss'])
epochs = range(1, len(history.history['acc']) + 1)

plt.plot(epochs, history.history['loss'])
plt.plot(epochs, history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

print("\n 정확도 : %.4f" % (model.evaluate(x_test, y_test)[1]))